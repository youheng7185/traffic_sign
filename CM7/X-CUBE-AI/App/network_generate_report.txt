ST Edge AI Core v2.1.0-20194 329b0e98d
Created date          : 2025-06-11 09:17:34
Parameters            : generate --target stm32h7 --name network -m C:/Users/lapchong/Downloads/quantized_gtsrb.tflite --compression none --verbosity 1 --workspace C:/Users/lapchong/AppData/Local/Temp/mxAI_workspace34466764514240012434307564274207824 --output C:/Users/lapchong/.stm32cubemx/network_output

Exec/report summary (generate)
----------------------------------------------------------------------------------------------------------------------------------
model file         :   C:\Users\lapchong\Downloads\quantized_gtsrb.tflite                                                         
type               :   tflite                                                                                                     
c_name             :   network                                                                                                    
compression        :   none                                                                                                       
options            :   allocate-inputs, allocate-outputs                                                                          
optimization       :   balanced                                                                                                   
target/series      :   stm32h7                                                                                                    
workspace dir      :   C:\Users\lapchong\AppData\Local\Temp\mxAI_workspace34466764514240012434307564274207824                     
output dir         :   C:\Users\lapchong\.stm32cubemx\network_output                                                              
model_fmt          :   sa/ua per tensor                                                                                           
model_name         :   quantized_gtsrb                                                                                            
model_hash         :   0xf5901b58a4779f212da49add7cdfc9a6                                                                         
params #           :   361,515 items (354.20 KiB)                                                                                 
----------------------------------------------------------------------------------------------------------------------------------
input 1/1          :   'serving_default_input_layer0', uint8(1x32x32x3), 3.00 KBytes, QLinear(0.003921569,0,uint8), activations   
output 1/1         :   'conversion_17', uint8(1x43), 43 Bytes, QLinear(0.003906250,0,uint8), activations                          
macc               :   10,768,870                                                                                                 
weights (ro)       :   362,700 B (354.20 KiB) (1 segment) / -1,083,360(-74.9%) vs float model                                     
activations (rw)   :   37,168 B (36.30 KiB) (1 segment) *                                                                         
ram (total)        :   37,168 B (36.30 KiB) = 37,168 + 0 + 0                                                                      
----------------------------------------------------------------------------------------------------------------------------------
(*) 'input'/'output' buffers can be used from the activations buffer

Model name - quantized_gtsrb
------ ------------------------------------------ ---------------------- ----------------- ----------- ------------------------------ --- --------------- ------------------- ---------------------- 
m_id   layer (type,original)                      oshape                 param/size               macc                   connected to   | c_size          c_macc              c_type                 
------ ------------------------------------------ ---------------------- ----------------- ----------- ------------------------------ --- --------------- ------------------- ---------------------- 
0      serving_default_input_layer0 (Input, )     [b:1,h:32,w:32,c:3]                                                                   |                 +6,144(+100.0%)     Conversion_[0]         
       conversion_0 (Conversion, QUANTIZE)        [b:1,h:32,w:32,c:3]                            6,144   serving_default_input_layer0   |                 -6,144(-100.0%)     
------ ------------------------------------------ ---------------------- ----------------- ----------- ------------------------------ --- --------------- ------------------- ---------------------- 
1      conv2d_1 (Conv2D, CONV_2D)                 [b:1,h:32,w:32,c:32]   896/992               884,768                   conversion_0   |                                     Conv2D_[1]             
       nl_1_nl (Nonlinearity, CONV_2D)            [b:1,h:32,w:32,c:32]                          32,768                       conv2d_1   |                 -32,768(-100.0%)    
------ ------------------------------------------ ---------------------- ----------------- ----------- ------------------------------ --- --------------- ------------------- ---------------------- 
2      tfl_pseudo_qconst13 (Placeholder, )        [b:32]                 32/32                                                          |                 +32,768(+100.0%)    Eltwise/mul_[2]        
       eltwise_2 (Eltwise, MUL)                   [b:1,h:32,w:32,c:32]                          32,768                        nl_1_nl   |                 -32,768(-100.0%)    
                                                                                                                  tfl_pseudo_qconst13   | 
------ ------------------------------------------ ---------------------- ----------------- ----------- ------------------------------ --- --------------- ------------------- ---------------------- 
3      tfl_pseudo_qconst12 (Placeholder, )        [b:32]                 32/32                                                          |                 +32,768(+100.0%)    Eltwise/add_[3]        
       eltwise_3 (Eltwise, ADD)                   [b:1,h:32,w:32,c:32]                          32,768                      eltwise_2   |                 -32,768(-100.0%)    
                                                                                                                  tfl_pseudo_qconst12   | 
------ ------------------------------------------ ---------------------- ----------------- ----------- ------------------------------ --- --------------- ------------------- ---------------------- 
4      pool_4 (Pool, MAX_POOL_2D)                 [b:1,h:16,w:16,c:32]                          32,768                      eltwise_3   |                                     Pool_[4]               
------ ------------------------------------------ ---------------------- ----------------- ----------- ------------------------------ --- --------------- ------------------- ---------------------- 
5      conv2d_5 (Conv2D, CONV_2D)                 [b:1,h:16,w:16,c:64]   18,496/18,688       4,718,656                         pool_4   |                                     Pad_/Conv2D_[5, 6]     
       nl_5_nl (Nonlinearity, CONV_2D)            [b:1,h:16,w:16,c:64]                          16,384                       conv2d_5   |                 -16,384(-100.0%)    
------ ------------------------------------------ ---------------------- ----------------- ----------- ------------------------------ --- --------------- ------------------- ---------------------- 
6      tfl_pseudo_qconst9 (Placeholder, )         [b:64]                 64/64                                                          |                 +16,384(+100.0%)    Eltwise/mul_[7]        
       eltwise_6 (Eltwise, MUL)                   [b:1,h:16,w:16,c:64]                          16,384                        nl_5_nl   |                 -16,384(-100.0%)    
                                                                                                                   tfl_pseudo_qconst9   | 
------ ------------------------------------------ ---------------------- ----------------- ----------- ------------------------------ --- --------------- ------------------- ---------------------- 
7      tfl_pseudo_qconst8 (Placeholder, )         [b:64]                 64/64                                                          |                 +16,384(+100.0%)    Eltwise/add_[8]        
       eltwise_7 (Eltwise, ADD)                   [b:1,h:16,w:16,c:64]                          16,384                      eltwise_6   |                 -16,384(-100.0%)    
                                                                                                                   tfl_pseudo_qconst8   | 
------ ------------------------------------------ ---------------------- ----------------- ----------- ------------------------------ --- --------------- ------------------- ---------------------- 
8      pool_8 (Pool, MAX_POOL_2D)                 [b:1,h:8,w:8,c:64]                            16,384                      eltwise_7   |                                     Pool_[9]               
------ ------------------------------------------ ---------------------- ----------------- ----------- ------------------------------ --- --------------- ------------------- ---------------------- 
9      conv2d_9 (Conv2D, CONV_2D)                 [b:1,h:8,w:8,c:128]    73,856/74,240       4,718,720                         pool_8   |                                     Pad_/Conv2D_[10, 11]   
       nl_9_nl (Nonlinearity, CONV_2D)            [b:1,h:8,w:8,c:128]                            8,192                       conv2d_9   |                 -8,192(-100.0%)     
------ ------------------------------------------ ---------------------- ----------------- ----------- ------------------------------ --- --------------- ------------------- ---------------------- 
10     tfl_pseudo_qconst5 (Placeholder, )         [b:128]                128/128                                                        |                 +8,192(+100.0%)     Eltwise/mul_[12]       
       eltwise_10 (Eltwise, MUL)                  [b:1,h:8,w:8,c:128]                            8,192                        nl_9_nl   |                 -8,192(-100.0%)     
                                                                                                                   tfl_pseudo_qconst5   | 
------ ------------------------------------------ ---------------------- ----------------- ----------- ------------------------------ --- --------------- ------------------- ---------------------- 
11     tfl_pseudo_qconst4 (Placeholder, )         [b:128]                128/128                                                        |                 +8,192(+100.0%)     Eltwise/add_[13]       
       eltwise_11 (Eltwise, ADD)                  [b:1,h:8,w:8,c:128]                            8,192                     eltwise_10   |                 -8,192(-100.0%)     
                                                                                                                   tfl_pseudo_qconst4   | 
------ ------------------------------------------ ---------------------- ----------------- ----------- ------------------------------ --- --------------- ------------------- ---------------------- 
12     pool_12 (Pool, MAX_POOL_2D)                [b:1,h:4,w:4,c:128]                            8,192                     eltwise_11   |                                     Pool_[14]              
------ ------------------------------------------ ---------------------- ----------------- ----------- ------------------------------ --- --------------- ------------------- ---------------------- 
13     reshape_13 (Reshape, RESHAPE)              [b:1,c:2048]                                                                pool_12   |                                     
------ ------------------------------------------ ---------------------- ----------------- ----------- ------------------------------ --- --------------- ------------------- ---------------------- 
14     tfl_pseudo_qconst3 (Placeholder, )         [b:128,c:2048]         262,144/262,144                                                | +512(+0.2%)     +262,272(+100.0%)   Dense_[15]             
       tfl_pseudo_qconst2 (Placeholder, )         [b:128]                128/512                                                        | -512(-100.0%)                       
       gemm_14 (Gemm, FULLY_CONNECTED)            [b:1,c:128]                                  262,272                     reshape_13   |                 -262,272(-100.0%)   
                                                                                                                   tfl_pseudo_qconst3   | 
                                                                                                                   tfl_pseudo_qconst2   | 
       nl_14_nl (Nonlinearity, FULLY_CONNECTED)   [b:1,c:128]                                      128                        gemm_14   |                 -128(-100.0%)       
------ ------------------------------------------ ---------------------- ----------------- ----------- ------------------------------ --- --------------- ------------------- ---------------------- 
15     tfl_pseudo_qconst1 (Placeholder, )         [b:43,c:128]           5,504/5,504                                                    | +172(+3.1%)     +5,547(+100.0%)     Dense_[16]             
       tfl_pseudo_qconst (Placeholder, )          [b:43]                 43/172                                                         | -172(-100.0%)                       
       gemm_15 (Gemm, FULLY_CONNECTED)            [b:1,c:43]                                     5,547                       nl_14_nl   |                 -5,547(-100.0%)     
                                                                                                                   tfl_pseudo_qconst1   | 
                                                                                                                    tfl_pseudo_qconst   | 
------ ------------------------------------------ ---------------------- ----------------- ----------- ------------------------------ --- --------------- ------------------- ---------------------- 
16     nl_16 (Nonlinearity, SOFTMAX)              [b:1,c:43]                                       645                        gemm_15   |                                     Nonlinearity_[17]      
------ ------------------------------------------ ---------------------- ----------------- ----------- ------------------------------ --- --------------- ------------------- ---------------------- 
17     conversion_17 (Conversion, QUANTIZE)       [b:1,c:43]                                        86                          nl_16   |                                     Conversion_[o][18]     
------ ------------------------------------------ ---------------------- ----------------- ----------- ------------------------------ --- --------------- ------------------- ---------------------- 
model/c-model: macc=10,826,342/10,768,870 -57,472(-0.5%) weights=362,700/362,700  activations=--/37,168 io=--/0



Generated C-graph summary
------------------------------------------------------------------------------------------------------------------------
model name            : quantized_gtsrb
c-name                : network
c-node #              : 19
c-array #             : 42
activations size      : 37168 (1 segment)
weights size          : 362700 (1 segment)
macc                  : 10768870
inputs                : ['serving_default_input_layer0_output']
outputs               : ['conversion_17_output']

C-Arrays (42)
------ ------------------------------------- --------------- ------------------------- ----------- --------- 
c_id   name (*_array)                        item/size       domain/mem-pool           c-type      comment   
------ ------------------------------------- --------------- ------------------------- ----------- --------- 
0      conv2d_1_bias                         32/128          weights/weights           const s32             
1      conv2d_1_output                       32768/32768     activations/**default**   s8                    
2      conv2d_1_scratch0                     2284/2284       activations/**default**   s8                    
3      conv2d_1_weights                      864/864         weights/weights           const s8              
4      conv2d_5_bias                         64/256          weights/weights           const s32             
5      conv2d_5_output                       16384/16384     activations/**default**   s8                    
6      conv2d_5_pad_before_output            10368/10368     activations/**default**   s8                    
7      conv2d_5_scratch0                     7168/7168       activations/**default**   s8                    
8      conv2d_5_weights                      18432/18432     weights/weights           const s8              
9      conv2d_9_bias                         128/512         weights/weights           const s32             
10     conv2d_9_output                       8192/8192       activations/**default**   s8                    
11     conv2d_9_pad_before_output            6400/6400       activations/**default**   s8                    
12     conv2d_9_scratch0                     9216/9216       activations/**default**   s8                    
13     conv2d_9_weights                      73728/73728     weights/weights           const s8              
14     conversion_0_output                   3072/3073       activations/**default**   s8                    
15     conversion_17_output                  43/43           activations/**default**   u8          /output   
16     eltwise_10_output                     8192/8192       activations/**default**   s8                    
17     eltwise_11_output                     8192/8192       activations/**default**   s8                    
18     eltwise_2_output                      32768/32768     activations/**default**   s8                    
19     eltwise_3_output                      32768/32768     activations/**default**   s8                    
20     eltwise_6_output                      16384/16384     activations/**default**   s8                    
21     eltwise_7_output                      16384/16384     activations/**default**   s8                    
22     gemm_14_bias                          128/512         weights/weights           const s32             
23     gemm_14_output                        128/128         activations/**default**   s8                    
24     gemm_14_scratch0                      2688/5376       activations/**default**   s16                   
25     gemm_14_weights                       262144/262144   weights/weights           const s8              
26     gemm_15_bias                          43/172          weights/weights           const s32             
27     gemm_15_output                        43/43           activations/**default**   s8                    
28     gemm_15_scratch0                      343/686         activations/**default**   s16                   
29     gemm_15_weights                       5504/5504       weights/weights           const s8              
30     nl_16_output                          43/43           activations/**default**   s8                    
31     nl_16_scratch0                        15/60           activations/**default**   s32                   
32     pool_12_output                        2048/2048       activations/**default**   s8                    
33     pool_4_output                         8192/8192       activations/**default**   s8                    
34     pool_8_output                         4096/4096       activations/**default**   s8                    
35     serving_default_input_layer0_output   3072/3072       activations/**default**   u8          /input    
36     tfl_pseudo_qconst12_4D                32/32           weights/weights           const s8              
37     tfl_pseudo_qconst13_4D                32/32           weights/weights           const s8              
38     tfl_pseudo_qconst4_4D                 128/128         weights/weights           const s8              
39     tfl_pseudo_qconst5_4D                 128/128         weights/weights           const s8              
40     tfl_pseudo_qconst8_4D                 64/64           weights/weights           const s8              
41     tfl_pseudo_qconst9_4D                 64/64           weights/weights           const s8              
------ ------------------------------------- --------------- ------------------------- ----------- --------- 

C-Layers (19)
------ --------------------- ---- --------------- --------- -------- ---------------------------------------- ----------------------- 
c_id   name (*_layer)        id   layer_type      macc      rom      tensors                                  shape (array id)        
------ --------------------- ---- --------------- --------- -------- ---------------------------------------- ----------------------- 
0      conversion_0          0    Conversion      6144      0        I: serving_default_input_layer0_output   uint8(1x32x32x3) (35)   
                                                                     O: conversion_0_output                   int8(1x32x32x3) (14)    
------ --------------------- ---- --------------- --------- -------- ---------------------------------------- ----------------------- 
1      conv2d_1              1    Conv2D          884768    992      I: conversion_0_output                   int8(1x32x32x3) (14)    
                                                                     S: conv2d_1_scratch0                                             
                                                                     W: conv2d_1_weights                      int8(32x3x3x3) (3)      
                                                                     W: conv2d_1_bias                         int32(32) (0)           
                                                                     O: conv2d_1_output                       int8(1x32x32x32) (1)    
------ --------------------- ---- --------------- --------- -------- ---------------------------------------- ----------------------- 
2      eltwise_2             2    Eltwise/mul     32768     32       I: conv2d_1_output                       int8(1x32x32x32) (1)    
                                                                     W: tfl_pseudo_qconst13_4D                int8(1x1x32) (37)       
                                                                     O: eltwise_2_output                      int8(1x32x32x32) (18)   
------ --------------------- ---- --------------- --------- -------- ---------------------------------------- ----------------------- 
3      eltwise_3             3    Eltwise/add     32768     32       I: eltwise_2_output                      int8(1x32x32x32) (18)   
                                                                     W: tfl_pseudo_qconst12_4D                int8(1x1x32) (36)       
                                                                     O: eltwise_3_output                      int8(1x32x32x32) (19)   
------ --------------------- ---- --------------- --------- -------- ---------------------------------------- ----------------------- 
4      pool_4                4    Pool            32768     0        I: eltwise_3_output                      int8(1x32x32x32) (19)   
                                                                     O: pool_4_output                         int8(1x16x16x32) (33)   
------ --------------------- ---- --------------- --------- -------- ---------------------------------------- ----------------------- 
5      conv2d_5_pad_before   5    Pad             0         0        I: pool_4_output                         int8(1x16x16x32) (33)   
                                                                     O: conv2d_5_pad_before_output            int8(1x18x18x32) (6)    
------ --------------------- ---- --------------- --------- -------- ---------------------------------------- ----------------------- 
6      conv2d_5              5    Conv2D          4718656   18688    I: conv2d_5_pad_before_output            int8(1x18x18x32) (6)    
                                                                     S: conv2d_5_scratch0                                             
                                                                     W: conv2d_5_weights                      int8(64x3x3x32) (8)     
                                                                     W: conv2d_5_bias                         int32(64) (4)           
                                                                     O: conv2d_5_output                       int8(1x16x16x64) (5)    
------ --------------------- ---- --------------- --------- -------- ---------------------------------------- ----------------------- 
7      eltwise_6             6    Eltwise/mul     16384     64       I: conv2d_5_output                       int8(1x16x16x64) (5)    
                                                                     W: tfl_pseudo_qconst9_4D                 int8(1x1x64) (41)       
                                                                     O: eltwise_6_output                      int8(1x16x16x64) (20)   
------ --------------------- ---- --------------- --------- -------- ---------------------------------------- ----------------------- 
8      eltwise_7             7    Eltwise/add     16384     64       I: eltwise_6_output                      int8(1x16x16x64) (20)   
                                                                     W: tfl_pseudo_qconst8_4D                 int8(1x1x64) (40)       
                                                                     O: eltwise_7_output                      int8(1x16x16x64) (21)   
------ --------------------- ---- --------------- --------- -------- ---------------------------------------- ----------------------- 
9      pool_8                8    Pool            16384     0        I: eltwise_7_output                      int8(1x16x16x64) (21)   
                                                                     O: pool_8_output                         int8(1x8x8x64) (34)     
------ --------------------- ---- --------------- --------- -------- ---------------------------------------- ----------------------- 
10     conv2d_9_pad_before   9    Pad             0         0        I: pool_8_output                         int8(1x8x8x64) (34)     
                                                                     O: conv2d_9_pad_before_output            int8(1x10x10x64) (11)   
------ --------------------- ---- --------------- --------- -------- ---------------------------------------- ----------------------- 
11     conv2d_9              9    Conv2D          4718720   74240    I: conv2d_9_pad_before_output            int8(1x10x10x64) (11)   
                                                                     S: conv2d_9_scratch0                                             
                                                                     W: conv2d_9_weights                      int8(128x3x3x64) (13)   
                                                                     W: conv2d_9_bias                         int32(128) (9)          
                                                                     O: conv2d_9_output                       int8(1x8x8x128) (10)    
------ --------------------- ---- --------------- --------- -------- ---------------------------------------- ----------------------- 
12     eltwise_10            10   Eltwise/mul     8192      128      I: conv2d_9_output                       int8(1x8x8x128) (10)    
                                                                     W: tfl_pseudo_qconst5_4D                 int8(1x1x128) (39)      
                                                                     O: eltwise_10_output                     int8(1x8x8x128) (16)    
------ --------------------- ---- --------------- --------- -------- ---------------------------------------- ----------------------- 
13     eltwise_11            11   Eltwise/add     8192      128      I: eltwise_10_output                     int8(1x8x8x128) (16)    
                                                                     W: tfl_pseudo_qconst4_4D                 int8(1x1x128) (38)      
                                                                     O: eltwise_11_output                     int8(1x8x8x128) (17)    
------ --------------------- ---- --------------- --------- -------- ---------------------------------------- ----------------------- 
14     pool_12               12   Pool            8192      0        I: eltwise_11_output                     int8(1x8x8x128) (17)    
                                                                     O: pool_12_output                        int8(1x4x4x128) (32)    
------ --------------------- ---- --------------- --------- -------- ---------------------------------------- ----------------------- 
15     gemm_14               14   Dense           262272    262656   I: pool_12_output                        int8(1x4x4x128) (32)    
                                                                     S: gemm_14_scratch0                                              
                                                                     W: gemm_14_weights                       int8(128x2048) (25)     
                                                                     W: gemm_14_bias                          int32(128) (22)         
                                                                     O: gemm_14_output                        int8(1x128) (23)        
------ --------------------- ---- --------------- --------- -------- ---------------------------------------- ----------------------- 
16     gemm_15               15   Dense           5547      5676     I: gemm_14_output                        int8(1x128) (23)        
                                                                     S: gemm_15_scratch0                                              
                                                                     W: gemm_15_weights                       int8(43x128) (29)       
                                                                     W: gemm_15_bias                          int32(43) (26)          
                                                                     O: gemm_15_output                        int8(1x43) (27)         
------ --------------------- ---- --------------- --------- -------- ---------------------------------------- ----------------------- 
17     nl_16                 16   Nonlinearity    645       0        I: gemm_15_output                        int8(1x43) (27)         
                                                                     S: nl_16_scratch0                                                
                                                                     O: nl_16_output                          int8(1x43) (30)         
------ --------------------- ---- --------------- --------- -------- ---------------------------------------- ----------------------- 
18     conversion_17         17   Conversion      86        0        I: nl_16_output                          int8(1x43) (30)         
                                                                     O: conversion_17_output                  uint8(1x43) (15)        
------ --------------------- ---- --------------- --------- -------- ---------------------------------------- ----------------------- 



Number of operations per c-layer
------- ------ ---------------------------- ------------ ------------ 
c_id    m_id   name (type)                           #op         type 
------- ------ ---------------------------- ------------ ------------ 
0       0      conversion_0 (Conversion)           6,144   smul_u8_s8 
1       1      conv2d_1 (Conv2D)                 884,768   smul_s8_s8 
2       2      eltwise_2 (Eltwise/mul)            32,768     op_s8_s8 
3       3      eltwise_3 (Eltwise/add)            32,768     op_s8_s8 
4       4      pool_4 (Pool)                      32,768   smul_s8_s8 
5       5      conv2d_5_pad_before (Pad)               0   smul_s8_s8 
6       5      conv2d_5 (Conv2D)               4,718,656   smul_s8_s8 
7       6      eltwise_6 (Eltwise/mul)            16,384     op_s8_s8 
8       7      eltwise_7 (Eltwise/add)            16,384     op_s8_s8 
9       8      pool_8 (Pool)                      16,384   smul_s8_s8 
10      9      conv2d_9_pad_before (Pad)               0   smul_s8_s8 
11      9      conv2d_9 (Conv2D)               4,718,720   smul_s8_s8 
12      10     eltwise_10 (Eltwise/mul)            8,192     op_s8_s8 
13      11     eltwise_11 (Eltwise/add)            8,192     op_s8_s8 
14      12     pool_12 (Pool)                      8,192   smul_s8_s8 
15      14     gemm_14 (Dense)                   262,272   smul_s8_s8 
16      15     gemm_15 (Dense)                     5,547   smul_s8_s8 
17      16     nl_16 (Nonlinearity)                  645     op_s8_s8 
18      17     conversion_17 (Conversion)             86   smul_s8_u8 
------- ------ ---------------------------- ------------ ------------ 
total                                         10,768,870 

Number of operation types
---------------- ------------ ----------- 
operation type              #           % 
---------------- ------------ ----------- 
smul_u8_s8              6,144        0.1% 
smul_s8_s8         10,647,307       98.9% 
op_s8_s8              115,333        1.1% 
smul_s8_u8                 86        0.0% 

Complexity report (model)
------ ------------------------------ ------------------------- ------------------------- ---------- 
m_id   name                           c_macc                    c_rom                     c_id       
------ ------------------------------ ------------------------- ------------------------- ---------- 
0      serving_default_input_layer0   |                  0.1%   |                  0.0%   [0]        
1      conv2d_1                       |||                8.2%   |                  0.3%   [1]        
2      tfl_pseudo_qconst13            |                  0.3%   |                  0.0%   [2]        
3      tfl_pseudo_qconst12            |                  0.3%   |                  0.0%   [3]        
4      pool_4                         |                  0.3%   |                  0.0%   [4]        
5      conv2d_5                       |||||||||||||||   43.8%   ||                 5.2%   [5, 6]     
6      tfl_pseudo_qconst9             |                  0.2%   |                  0.0%   [7]        
7      tfl_pseudo_qconst8             |                  0.2%   |                  0.0%   [8]        
8      pool_8                         |                  0.2%   |                  0.0%   [9]        
9      conv2d_9                       ||||||||||||||||  43.8%   |||||             20.5%   [10, 11]   
10     tfl_pseudo_qconst5             |                  0.1%   |                  0.0%   [12]       
11     tfl_pseudo_qconst4             |                  0.1%   |                  0.0%   [13]       
12     pool_12                        |                  0.1%   |                  0.0%   [14]       
14     tfl_pseudo_qconst3             |                  2.4%   ||||||||||||||||  72.4%   [15]       
15     tfl_pseudo_qconst1             |                  0.1%   |                  1.6%   [16]       
16     nl_16                          |                  0.0%   |                  0.0%   [17]       
17     conversion_17                  |                  0.0%   |                  0.0%   [18]       
------ ------------------------------ ------------------------- ------------------------- ---------- 
macc=10,768,870 weights=362,700 act=37,168 ram_io=0
 
 Requested memory size by section - "stm32h7" target
 ------------------------------ -------- --------- ------- -------- 
 module                             text    rodata    data      bss 
 ------------------------------ -------- --------- ------- -------- 
 NetworkRuntime1010_CM7_GCC.a     28,892         0       0        0 
 network.o                         1,016     2,815   5,516      300 
 network_data.o                       48        16      88        0 
 lib (toolchain)*                    104         0       0        0 
 ------------------------------ -------- --------- ------- -------- 
 RT total**                       30,060     2,831   5,604      300 
 ------------------------------ -------- --------- ------- -------- 
 weights                               0   362,704       0        0 
 activations                           0         0       0   37,168 
 io                                    0         0       0        0 
 ------------------------------ -------- --------- ------- -------- 
 TOTAL                            30,060   365,535   5,604   37,468 
 ------------------------------ -------- --------- ------- -------- 
 *  toolchain objects (libm/libgcc*)
 ** RT AI runtime objects (kernels+infrastructure)
  
  Summary - "stm32h7" target
  --------------------------------------------------
               FLASH (ro)     %*   RAM (rw)       % 
  --------------------------------------------------
  RT total         38,495   9.6%      5,904   13.7% 
  --------------------------------------------------
  TOTAL           401,199            43,072         
  --------------------------------------------------
  *  rt/total


Generated files (7)
--------------------------------------------------------------------- 
C:\Users\lapchong\.stm32cubemx\network_output\network_data_params.h   
C:\Users\lapchong\.stm32cubemx\network_output\network_data_params.c   
C:\Users\lapchong\.stm32cubemx\network_output\network_data.h          
C:\Users\lapchong\.stm32cubemx\network_output\network_data.c          
C:\Users\lapchong\.stm32cubemx\network_output\network_config.h        
C:\Users\lapchong\.stm32cubemx\network_output\network.h               
C:\Users\lapchong\.stm32cubemx\network_output\network.c               
